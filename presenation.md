# Crawling The Web: A Deep Dive into Data Collection
## Simeon Martev 
- https://www.linkedin.com/in/smartev/
- https://identrics.ai/

## 1. Introduction to My Work and the Importance of Data Collection
- My role as VP of Engineering at Identrics
- Me and my team colect: media data, data about sanctioned people and organizations, crime data, and financial data.
- The importance of data in today's digital world.
- Case study: How the use of crawled data helped a business or agency make a critical decision.

## 2. Tools Used for Data Collection
- Python https://www.python.org/ 
- Scrapy https://scrapy.org/ 
- PostgreSQL https://www.postgresql.org/ 
- Elasticsearch, Kibana, Logstash https://www.elastic.co/
- RabbitMQ https://www.rabbitmq.com/
- Airflow https://airflow.apache.org/
- Django https://www.djangoproject.com/
- Why these tools? A comparison with other popular tools.


## 3. Understanding HTTP, XPath, Scrapy CSS Selector
- Basics of HTTP: what it is, why it matters for data collection. https://developer.mozilla.org/en-US/docs/Web/HTTP
    - GET
    - POST
- XPath and Scrapy CSS selector: Their role in navigating and extracting data. https://developer.mozilla.org/en-US/docs/Web/XPath

## 4. Data Formats: HTML, JSON, XML, RSS Feed, API
- What is HTML: Brief overview and its significance in web data collection.
    - https://developer.mozilla.org/en-US/docs/Web/HTML
- JSON and XML: Their role in data interchange.
    - https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON 
    - https://developer.mozilla.org/en-US/docs/Web/XML/XML_introduction
- Understanding RSS Feed and APIs: How they facilitate data collection.
    - https://developer.mozilla.org/en-US/docs/Glossary/RSS

## 5. Hands-on with Scrapy: An Open Source Web Crawling Framework
- Introduction to Scrapy: What it is and why we use it.
    - https://scapy.readthedocs.io/en/latest/
- Basic principles of working with Scrapy.
    - spiders
    - items
    - pipeline
- A walk-through: Building a simple Scrapy spider (pre-prepared by you).
- Common challenges and how to overcome them.

## 6. Writing Data using Scrapy Pipeline in SQLite
- The role of Scrapy pipeline: Process and store scraped data.
- Introduction to SQLite: A lightweight database for storing data.
- Step-by-step guide: How to write scraped data into SQLite using Scrapy pipeline.
- Example: Store data scraped in the previous section into SQLite.

## 7. Q&A Session ðŸ¤”